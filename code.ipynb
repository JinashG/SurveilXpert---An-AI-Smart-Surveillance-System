{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import threading\n",
    "import cv2\n",
    "from scipy.io.wavfile import write\n",
    "from scipy.signal import butter, lfilter\n",
    "from ultralytics import YOLO\n",
    "from datetime import datetime\n",
    "from deepface import DeepFace\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pymongo import MongoClient\n",
    "from socketio import Client\n",
    "\n",
    "# Initialize Socket.IO client with logging\n",
    "sio = Client(logger=True, engineio_logger=True)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize MongoDB client\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"SurveilXpertDB\"]\n",
    "alerts_collection = db[\"alerts\"]\n",
    "\n",
    "# === Model Loading ===\n",
    "yolo_model_path = 'Yolo_nano_weights.pt'\n",
    "\n",
    "# Load models\n",
    "audio_model = tf.keras.models.load_model(\"best_model1.keras\")\n",
    "\n",
    "# Download YAMNet model locally if not already downloaded\n",
    "yamnet_model_path = \"yamnet_model\"\n",
    "if not os.path.exists(yamnet_model_path):\n",
    "    os.makedirs(yamnet_model_path)\n",
    "    \n",
    "    # Download the model\n",
    "    url = \"https://tfhub.dev/google/yamnet/1?tf-hub-format=compressed\"\n",
    "    response = requests.get(url)\n",
    "    with open(os.path.join(yamnet_model_path, \"yamnet.tar.gz\"), \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    # Extract the model\n",
    "    with tarfile.open(os.path.join(yamnet_model_path, \"yamnet.tar.gz\"), \"r:gz\") as tar:\n",
    "        tar.extractall(path=yamnet_model_path)\n",
    "\n",
    "# Load YAMNet model from local path\n",
    "yamnet_model = hub.load(yamnet_model_path)\n",
    "# yamnet_model = hub.load('https://www.kaggle.com/models/google/yamnet/tensorFlow2/yamnet/1?tfhub-redirect=true')\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "\n",
    "# Ensure required directories exist\n",
    "os.makedirs(\"anomalous_videos\", exist_ok=True)\n",
    "os.makedirs(\"anomalous_audio\", exist_ok=True)\n",
    "os.makedirs(\"unknown_faces\", exist_ok=True)\n",
    "\n",
    "# Index-to-label mapping for YAMNet\n",
    "index_to_label = {\n",
    "    0: \"Emergency_alert_sound\",\n",
    "    1: \"Explosions\",\n",
    "    2: \"Gunshots\",\n",
    "    3: \"Human screams\",\n",
    "    4: \"Bottles breaking\",\n",
    "    5: \"Dog bark\",\n",
    "    6: \"Human Conversation\",\n",
    "}\n",
    "\n",
    "# Noise reduction using a Butterworth filter\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut=300.0, highcut=3400.0, fs=16000, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Preprocess and generate YAMNet embeddings\n",
    "def preprocess_audio(audio_waveform, target_length=16000 * 5):\n",
    "    audio_waveform = bandpass_filter(audio_waveform)\n",
    "    if len(audio_waveform) < target_length:\n",
    "        audio_waveform = np.pad(audio_waveform, (0, target_length - len(audio_waveform)))\n",
    "    else:\n",
    "        audio_waveform = audio_waveform[:target_length]\n",
    "    audio_waveform = audio_waveform.astype(np.float32)\n",
    "    audio_waveform /= np.max(np.abs(audio_waveform))\n",
    "    return audio_waveform\n",
    "\n",
    "def predict_audio_from_stream(audio_data, sr, confidence_threshold=0.7):\n",
    "    processed_audio = preprocess_audio(audio_data)\n",
    "    _, yamnet_embeddings, _ = yamnet_model(processed_audio)\n",
    "    avg_embedding = tf.reduce_mean(yamnet_embeddings, axis=0).numpy().reshape(1, -1)\n",
    "    prediction = audio_model.predict(avg_embedding)\n",
    "    predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "    confidence = prediction[0][predicted_class_index]\n",
    "    if confidence >= confidence_threshold:\n",
    "        predicted_class_name = index_to_label.get(predicted_class_index, \"Unknown\")\n",
    "        return predicted_class_name, confidence\n",
    "    return \"Unknown\", confidence\n",
    "\n",
    "# Audio capturing and processing thread\n",
    "def process_audio_stream(duration=5, sample_rate=16000):\n",
    "    print(\"[INFO] Starting audio detection thread...\")\n",
    "    while True:\n",
    "        print(\"[INFO] Recording audio...\")\n",
    "        audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')\n",
    "        sd.wait()\n",
    "        audio_data = audio_data.flatten()\n",
    "\n",
    "        predicted_class, confidence = predict_audio_from_stream(audio_data, sample_rate)\n",
    "\n",
    "        if predicted_class != \"Unknown\":\n",
    "            print(f\"[{datetime.now()}] Detected audio anomaly: {predicted_class} (Confidence: {confidence:.2f})\")\n",
    "\n",
    "            if predicted_class == \"Human Conversation\":\n",
    "                print(\"[INFO] Detected 'Human Conversation'. Audio will not be recorded.\")\n",
    "                continue\n",
    "            if predicted_class == \"Explosions\":\n",
    "                continue\n",
    "\n",
    "            audio_filename = f\"anomalous_audio/{predicted_class}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav\"\n",
    "            write(audio_filename, sample_rate, (audio_data * 32767).astype(np.int16))\n",
    "            print(f\"[INFO] Audio saved to: {audio_filename}\")\n",
    "\n",
    "            alert = {\n",
    "                \"type\": \"audio\",\n",
    "                \"anomaly\": predicted_class,\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"file_path\": audio_filename\n",
    "            }\n",
    "            alerts_collection.insert_one(alert)\n",
    "            try:\n",
    "                print(\"[INFO] Emitting alert...\")\n",
    "                sio.emit('new_alert', alert)\n",
    "                print(\"[INFO] Alert emitted successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to emit alert: {e}\")\n",
    "\n",
    "# Face recognition setup\n",
    "known_faces_dir = \"known_faces\"\n",
    "os.makedirs(known_faces_dir, exist_ok=True)\n",
    "known_encodings = []\n",
    "known_names = []\n",
    "\n",
    "# Load known faces\n",
    "def load_known_faces():\n",
    "    for name in os.listdir(known_faces_dir):\n",
    "        person_dir = os.path.join(known_faces_dir, name)\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "        for file in os.listdir(person_dir):\n",
    "            file_path = os.path.join(person_dir, file)\n",
    "            try:\n",
    "                encoding = DeepFace.represent(img_path=file_path, model_name='Facenet')[0]['embedding']\n",
    "                known_encodings.append(encoding)\n",
    "                known_names.append(name)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading face {file_path}: {e}\")\n",
    "\n",
    "load_known_faces()\n",
    "\n",
    "def recognize_faces(frame):\n",
    "    try:\n",
    "        detections = DeepFace.find(img_path=frame, db_path=known_faces_dir, model_name='Facenet', enforce_detection=False)\n",
    "        if detections and not detections[0].empty:\n",
    "            return detections[0].iloc[0]['identity']\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during face recognition: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def is_face_saved(face_encoding, threshold=0.6):\n",
    "    for known_encoding in known_encodings:\n",
    "        similarity = cosine_similarity([face_encoding], [known_encoding])[0][0]\n",
    "        if similarity > threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Save the unknown face to the folder\n",
    "def save_unknown_face(frame):\n",
    "    try:\n",
    "        encoding = DeepFace.represent(img_path=frame, model_name='Facenet')[0]['embedding']\n",
    "        if not is_face_saved(encoding):\n",
    "            face_filename = f\"unknown_faces/unknown_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg\"\n",
    "            cv2.imwrite(face_filename, frame)\n",
    "            alert = {\n",
    "                \"type\": \"face\",\n",
    "                \"anomaly\": \"Unknown Face detected\",\n",
    "                \"timestamp\": datetime.now(),\n",
    "                \"file_path\": face_filename\n",
    "            }\n",
    "            alerts_collection.insert_one(alert)\n",
    "            try:\n",
    "                print(\"[INFO] Emitting face alert...\")\n",
    "                sio.emit('new_alert', alert)\n",
    "                print(\"[INFO] Face alert emitted successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to emit face alert: {e}\")\n",
    "            known_encodings.append(encoding)\n",
    "        else:\n",
    "            print(f\"[INFO] Duplicate unknown face detected, not saving.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving unknown face: {e}\")\n",
    "\n",
    "# Video threat detection\n",
    "def process_video_stream(video_source=0):\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video stream\")\n",
    "        return\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n",
    "\n",
    "    out = None\n",
    "    recording = False\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Warning: Failed to capture frame, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Perform face recognition\n",
    "        face_result = recognize_faces(frame)\n",
    "        if face_result == \"Unknown\":\n",
    "            print(f\"[{datetime.now()}] Unknown face detected!\")\n",
    "            save_unknown_face(frame)\n",
    "\n",
    "        # Perform object detection\n",
    "        results = yolo_model.predict(frame, conf=0.75, verbose=False)\n",
    "\n",
    "        anomalies_detected = False\n",
    "        if results[0].boxes:\n",
    "            for box in results[0].boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].numpy())\n",
    "                cls_id = int(box.cls)\n",
    "                confidence = box.conf.item()\n",
    "                label = yolo_model.names[cls_id]\n",
    "\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"{label} ({confidence:.2f})\",\n",
    "                    (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0, 255, 255),\n",
    "                    2,\n",
    "                )\n",
    "\n",
    "                print(f\"[{datetime.now()}] Detected: {label} (Confidence: {confidence:.2f})\")\n",
    "                if label in [\"violence\", \"weaponized\"]:\n",
    "                    anomalies_detected = True\n",
    "                    if not recording:\n",
    "                        video_filename = f\"anomalous_videos/{label}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4\"\n",
    "                        out = cv2.VideoWriter(video_filename, fourcc, fps, (frame_width, frame_height))\n",
    "                        recording = True\n",
    "                        print(f\"[{datetime.now()}] Recording started: {video_filename}\")\n",
    "                        alert = {\n",
    "                            \"type\": \"video\",\n",
    "                            \"anomaly\": label,\n",
    "                            \"confidence\": confidence,\n",
    "                            \"timestamp\": datetime.now(),\n",
    "                            \"file_path\": video_filename\n",
    "                        }\n",
    "                        alerts_collection.insert_one(alert)\n",
    "                        try:\n",
    "                            print(\"[INFO] Emitting video alert...\")\n",
    "                            sio.emit('new_alert', alert)\n",
    "                            print(\"[INFO] Video alert emitted successfully\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"[ERROR] Failed to emit video alert: {e}\")\n",
    "\n",
    "            if recording and out:\n",
    "                out.write(frame)\n",
    "\n",
    "        if not anomalies_detected and recording:\n",
    "            print(f\"[{datetime.now()}] Anomaly ended. Stopping recording.\")\n",
    "            recording = False\n",
    "            if out:\n",
    "                out.release()\n",
    "                out = None\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Exit requested. Stopping program.\")\n",
    "            break\n",
    "\n",
    "    if recording and out:\n",
    "        out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Unified system\n",
    "def unified_system(video_source=0):\n",
    "    video_thread = threading.Thread(target=process_video_stream, args=(video_source,))\n",
    "    audio_thread = threading.Thread(target=process_audio_stream)\n",
    "\n",
    "    video_thread.start()\n",
    "    audio_thread.start()\n",
    "\n",
    "    video_thread.join()\n",
    "    audio_thread.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unified_system()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
